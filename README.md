# ğŸ¤–ğŸ“š Sci-Fi History RAG API

Um microserviÃ§o em Python que utiliza **LangChain**, **Pinecone**, **FastAPI** e **OpenAI** para responder perguntas sobre a histÃ³ria da ficÃ§Ã£o cientÃ­fica com base em um livro indexado.

> ğŸ” Powered by Retrieval-Augmented Generation (RAG) para oferecer respostas contextualizadas e precisas.

## âœ¨ Funcionalidades

- ğŸ” **RAG (Retrieval-Augmented Generation)** com LangChain
- ğŸ“– Baseado em um livro sobre a histÃ³ria da ficÃ§Ã£o cientÃ­fica
- ğŸ§  VetorizaÃ§Ã£o com Pinecone
- âš¡ API REST com FastAPI
- ğŸ¤– IntegraÃ§Ã£o com modelos da OpenAI (GPT)
- ğŸ§ª Testes prontos para validar endpoints e lÃ³gica

## ğŸ“š Sobre o projeto

Este projeto responde perguntas em linguagem natural sobre obras, autores e movimentos histÃ³ricos da ficÃ§Ã£o cientÃ­fica, extraindo dados diretamente de um livro processado e indexado. Ideal para:

- Estudantes e pesquisadores
- Leitores apaixonados por sci-fi
- AplicaÃ§Ãµes educacionais

## ğŸ§° Tecnologias Utilizadas

| Tecnologia  | DescriÃ§Ã£o                                      |
|-------------|------------------------------------------------|
| ğŸ Python 3  | Linguagem principal                            |
| âš¡ FastAPI   | CriaÃ§Ã£o da API REST                            |
| ğŸ”— LangChain | Framework para RAG e pipelines de LLMs         |
| ğŸŒ² Pinecone  | Banco vetorial para buscas semÃ¢nticas          |
| ğŸ§  OpenAI    | GeraÃ§Ã£o de linguagem natural com modelos LLM   |

---

## ğŸš€ Como rodar localmente

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/sci-fi-history-rag-api.git
cd sci-fi-history-rag-api
````

### 2. Instale as dependÃªncias

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Instale o Ollama com o modelo Llama3.1:8b

Basta executar o comando:

`docker-compose up --build -d`

E por fim rodar o comando:

`docker exec -it ollama ollama pull llama3.1:8b`

Ou entÃ£o, instale manualmente o Ollama e baixe o modelo llama3.1 com o comando `ollama pull llama3.1:8b`.

### 4. Configure as variÃ¡veis de ambiente

Crie um arquivo `.env` com:

```env
OPENAI_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
PINECONE_API_KEY=pcsk_...
PINECONE_INDEX=scifi-rag
PROJECT_ROOT=.
DEFAULT_VECTOR_FILTER_ID=36eafca6-5848-4e58-be2c-fdc8888588ff
```

### 5. Rode o servidor

```bash
uvicorn app:app --reload
```

---

## ğŸ“¥ Exemplo de uso

### `POST /api/query`

```json
{
  "query": "Liste os livros de Isaac Asimov que foram citados",
  "filter_id": "36eafca6-5848-4e58-be2c-fdc8888588ff",
  "top_k": 15,
  "history": [
    "user: Isaac Asimov Ã© citado no documento?",
    "assistant: Sim, alguns livros de Isaac Asimov sÃ£o citados!"
  ]
}
```

### Resposta

```json
{
  "content": "Os livros de Isaac Asimov que foram citados nos documentos sÃ£o:\n\n1. **The Bicentennial Man** (1976) - IncluÃ­do em \"The Complete Robot\".\n2. **The Caves of Steel** (1954).",
  "total_docs": 15,
  "usage": {
    "input_tokens": 10490,
    "output_tokens": 50,
    "total_tokens": 10540
  },
  "docs": [
    "caÃ³tico. E embora nÃ£o possamos supor que Asimov tenha previsto a Teoria do\nCaos, o fato Ã© que na sÃ©rie Duna,...",
    "determinante original da FC parece tender para a vertente materialista ou\nprotestante; mas de fato aquela vertente catÃ³lica mÃ­stica/fantÃ¡stica estÃ¡ muito...",
    "e Peter Gallison, com Amy Slaton (orgs.). Picturing Science and Producing Art. Nova..."
  ]
}
```


## ğŸ“Œ Estrutura do projeto

```
openai-scifi-rag/
â”œâ”€â”€ files/
â”‚   â””â”€â”€ scifi.txt                  # Texto de entrada usado no projeto
â”œâ”€â”€ app.py                         # Entrypoint da aplicaÃ§Ã£o FastAPI
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes gerais do projeto
â”‚   â”‚   â””â”€â”€ prompt.py              # Prompts customizados para o LLM
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llm_integration.py     # IntegraÃ§Ã£o com LLM (OpenAI ou local)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ endpoint.py            # Schemas para requests/responses da API
â”‚   â”‚   â””â”€â”€ llm_models.py          # Modelos auxiliares relacionados ao LLM
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ rag_route.py           # DefiniÃ§Ã£o de endpoints da API
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_service.py         # Regras de negÃ³cio do RAG
â”‚   â”œâ”€â”€ vector_store/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ vector_store.py        # IntegraÃ§Ã£o com o repositÃ³rio vetorial (ex: Pinecone)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt               # DependÃªncias do projeto
â””â”€â”€ README.md                      # InstruÃ§Ãµes e descriÃ§Ã£o do projeto
```

---

## ğŸ“– LicenÃ§a

[MIT](LICENSE)

---

## âœï¸ Autor

**Victor Hugo Negrisoli**
ğŸ”— [LinkedIn](https://www.linkedin.com/in/victorhugonegrisoli/) | ğŸ™ [GitHub](https://github.com/vhnegrisoli/)
